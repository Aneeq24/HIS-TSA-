{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33659962-9a0b-4cac-8d19-3184163cc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the working directory to the root\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce03d17-9b58-44ba-8aed-eb016815bc85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import os\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "np.random.seed()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47449e71-2fb2-43f6-92cc-9bb0190c098f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading data to pandas dataframe\n",
    "\n",
    "Let's use London Smart Meters dataset from Kaggle for the current exercise. The source of the data is from London Data Store and Jean-Michel D. has augumented the dataset with a few more details and packaged it as a Kaggle Dataset.\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "London Data Store, free and open data-sharing portal, provided this dataset, which was collected and enriched by Jean-Michel D and uploaded on Kaggle @ https://www.kaggle.com/jeanmidev/smart-meters-in-london. \n",
    "The dataset has energy consumption readings for a sample of 5,567 London Households that took part in the UK Power Networks led Low Carbon London project between November 2011 and February 2014. Readings were taken at half hourly intervals. Some metadata about the households are also available as part of the dataset.\n",
    "\n",
    "## Data Wrangling\n",
    "The Kaggle dataset also has the time series data preprocessed on a daily level and combined all the separate files, etc. But letâ€™s ignore those files and start with the raw files, which is in the hhblock_dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363d00d-7f4d-494a-8317-938e30a66590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"imgs/chapter_2\", exist_ok=True)\n",
    "source_data = Path(\"/Users/aa/Documents/Semester 3/HIS/Modern-Time-Series-Forecasting-with-Python/scripts/data/london_smart_meters/\")\n",
    "block_data_path = source_data/\"hhblock_dataset\"/\"hhblock_dataset\"\n",
    "\n",
    "print(source_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ed620-92a0-4539-a3c0-f23fe52d9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert block_data_path.is_dir(), \"Please check if the dataset has been downloaded properly. Refer to the Preface of the book or the Readme in the repo for expected data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a701f-df4e-4b25-b980-aebb0c57361f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Converting the half hourly block level dataset into a time series data \n",
    "\n",
    "Let's pick one block and see how we can transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1bfe9-fd32-4f13-8c53-2a5d12e66df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_1 = pd.read_csv(block_data_path/\"block_0.csv\", parse_dates=False)\n",
    "\n",
    "block_1['day'] = pd.to_datetime(block_1['day'], yearfirst=True)\n",
    "\n",
    "block_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc08a06-725f-45e4-ac22-d018a7d5317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check End Dates of all time series\n",
    "block_1.groupby(\"LCLid\")['day'].max().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cadd15d-1d8a-4779-b64b-db8ba612b3b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/aa/Documents/Semester 3/HIS/Modern-Time-Series-Forecasting-with-Python/notebooks/Chapter02/02 - Preprocessing London Smart Meter Dataset.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aa/Documents/Semester%203/HIS/Modern-Time-Series-Forecasting-with-Python/notebooks/Chapter02/02%20-%20Preprocessing%20London%20Smart%20Meter%20Dataset.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m max_date \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aa/Documents/Semester%203/HIS/Modern-Time-Series-Forecasting-with-Python/notebooks/Chapter02/02%20-%20Preprocessing%20London%20Smart%20Meter%20Dataset.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m tqdm(block_data_path\u001b[39m.\u001b[39mglob(\u001b[39m\"\u001b[39m\u001b[39m*.csv\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aa/Documents/Semester%203/HIS/Modern-Time-Series-Forecasting-with-Python/notebooks/Chapter02/02%20-%20Preprocessing%20London%20Smart%20Meter%20Dataset.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(f, parse_dates\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aa/Documents/Semester%203/HIS/Modern-Time-Series-Forecasting-with-Python/notebooks/Chapter02/02%20-%20Preprocessing%20London%20Smart%20Meter%20Dataset.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mday\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[\u001b[39m'\u001b[39m\u001b[39mday\u001b[39m\u001b[39m'\u001b[39m], yearfirst\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aa/Documents/Semester%203/HIS/Modern-Time-Series-Forecasting-with-Python/notebooks/Chapter02/02%20-%20Preprocessing%20London%20Smart%20Meter%20Dataset.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m max_date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "max_date = None\n",
    "for f in tqdm(block_data_path.glob(\"*.csv\")):\n",
    "    df = pd.read_csv(f, parse_dates=False)\n",
    "    df['day'] = pd.to_datetime(df['day'], yearfirst=True)\n",
    "    if max_date is None:\n",
    "        max_date = df['day'].max()\n",
    "    else:\n",
    "        if df['day'].max()>max_date:\n",
    "            max_date = df['day'].max()\n",
    "print(f\"Max Date across all blocks: {max_date}\")\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1271f2-201e-4544-95b7-f006bab2594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the dataframe into the long form with hour blocks along the rows\n",
    "block_1 = block_1.set_index(['LCLid', \"day\"]).stack().reset_index().rename(columns={\"level_2\": \"hour_block\", 0: \"energy_consumption\"})\n",
    "#Creating a numerical hourblock column\n",
    "block_1['offset'] = block_1['hour_block'].str.replace(\"hh_\", \"\").astype(int)\n",
    "\n",
    "block_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4e040-ea80-4353-939d-4fd7ff2a2880",
   "metadata": {},
   "source": [
    "#### Compact Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19551b7-b632-4865-90af-2abf172da3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_compact(x):\n",
    "    start_date = x['day'].min()\n",
    "    name = x['LCLid'].unique()[0]\n",
    "    ### Fill missing dates with NaN ###\n",
    "    # Create a date range from  min to max\n",
    "    dr = pd.date_range(start=x['day'].min(), end=max_date, freq=\"1D\")\n",
    "    # Add hh_0 to hh_47 to columns and with some unstack magic recreating date-hh_x combinations\n",
    "    dr = pd.DataFrame(columns=[f\"hh_{i}\" for i in range(48)], index=dr).unstack().reset_index()\n",
    "    # renaming the columns\n",
    "    dr.columns = [\"hour_block\", \"day\", \"_\"]\n",
    "    # left merging the dataframe to the standard dataframe\n",
    "    # now the missing values will be left as NaN\n",
    "    dr = dr.merge(x, on=['hour_block','day'], how='left')\n",
    "    # sorting the rows\n",
    "    dr.sort_values(['day',\"offset\"], inplace=True)\n",
    "    # extracting the timeseries array\n",
    "    ts = dr['energy_consumption'].values\n",
    "    len_ts = len(ts)\n",
    "    return start_date, name, ts, len_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbbf9b-6a29-4289-9ae0-2d311388a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_block_compact(block_df, freq=\"30min\", ts_identifier=\"series_name\", value_name=\"series_value\"):\n",
    "    grps = block_df.groupby('LCLid')\n",
    "    all_series = []\n",
    "    all_start_dates = []\n",
    "    all_names = []\n",
    "    all_data = {}\n",
    "    all_len = []\n",
    "    for idx, df in tqdm(grps, leave=False):\n",
    "        start_date, name, ts, len_ts = preprocess_compact(df)\n",
    "        all_series.append(ts)\n",
    "        all_start_dates.append(start_date)\n",
    "        all_names.append(name)\n",
    "        all_len.append(len_ts)\n",
    "\n",
    "    all_data[ts_identifier] = all_names\n",
    "    all_data['start_timestamp'] = all_start_dates\n",
    "    all_data['frequency'] = freq\n",
    "    all_data[value_name] = all_series\n",
    "    all_data['series_length'] = all_len\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "block1_compact = load_process_block_compact(block_1, freq=\"30min\", ts_identifier=\"LCLid\", value_name=\"energy_consumption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768098c-cb7b-42ff-9ff2-b09589953218",
   "metadata": {},
   "outputs": [],
   "source": [
    "block1_compact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22221006-26c7-4c11-92d4-7fbe6d4ec89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(block1_compact.memory_usage(deep=True))\n",
    "print(f\"Total: {block1_compact.memory_usage(deep=True).sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e178ac-5e36-466a-bc4c-a068c3fe89c3",
   "metadata": {},
   "source": [
    "#### Expanded Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf102637-d9c8-4869-9d58-4e1ace5de1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_expanded(x):\n",
    "    start_date = x['day'].min()\n",
    "    ### Fill missing dates with NaN ###\n",
    "    # Create a date range from  min to max\n",
    "    dr = pd.date_range(start=x['day'].min(), end=x['day'].max(), freq=\"1D\")\n",
    "    # Add hh_0 to hh_47 to columns and with some unstack magic recreating date-hh_x combinations\n",
    "    dr = pd.DataFrame(columns=[f\"hh_{i}\" for i in range(48)], index=dr).unstack().reset_index()\n",
    "    # renaming the columns\n",
    "    dr.columns = [\"hour_block\", \"day\", \"_\"]\n",
    "    # left merging the dataframe to the standard dataframe\n",
    "    # now the missing values will be left as NaN\n",
    "    dr = dr.merge(x, on=['hour_block','day'], how='left')\n",
    "    dr['series_length'] = len(dr)\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b89d9b-a6bc-413e-aef0-b33dd2a69f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_block_expanded(block_df, freq=\"30min\"):\n",
    "    grps = block_df.groupby('LCLid')\n",
    "    all_series = []\n",
    "    for idx, df in tqdm(grps, leave=False):\n",
    "        ts = preprocess_expanded(df)\n",
    "        all_series.append(ts)\n",
    "\n",
    "    block_df = pd.concat(all_series)\n",
    "    # Recreate Offset because there would be null rows now\n",
    "    block_df['offset'] = block_df['hour_block'].str.replace(\"hh_\", \"\").astype(int)\n",
    "    # Creating a datetime column with the date | Will take some time because operation is not vectorized\n",
    "    block_df['timestamp'] = block_df['day'] + block_df['offset']*30*pd.offsets.Minute()\n",
    "    block_df['frequency'] = freq\n",
    "    block_df.sort_values([\"LCLid\",\"timestamp\"], inplace=True)\n",
    "    block_df.drop(columns=[\"_\", \"hour_block\", \"offset\", \"day\"], inplace=True)\n",
    "    return block_df\n",
    "#     del all_series\n",
    "block1_expanded = load_process_block_expanded(block_1, freq=\"30min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a46c20-c802-4b28-a18f-0fc3bfe3fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "block1_expanded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ea3c3-94e4-4093-b9cf-77a8bc35198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(block1_expanded.memory_usage())\n",
    "print(f\"Total: {block1_expanded.memory_usage().sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76246e-bdc6-4ec9-b860-27da4071edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del block1_expanded, block_1, block1_compact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eaa201-7292-4f33-8ff2-d84e5b767fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reading and combining all the block data into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c68f5b-fe77-42a4-b375-ac6ef8f23862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_df_l = []\n",
    "for file in tqdm(sorted(list(block_data_path.glob(\"*.csv\"))), desc=\"Processing Blocks..\"):\n",
    "    block_df = pd.read_csv(file, parse_dates=False)\n",
    "    block_df['day'] = pd.to_datetime(block_df['day'], yearfirst=True)\n",
    "    # Taking only from 2012-01-01\n",
    "    block_df = block_df.loc[block_df['day']>=\"2012-01-01\"]\n",
    "    #Reshaping the dataframe into the long form with hour blocks along the rows\n",
    "    block_df = block_df.set_index(['LCLid', \"day\"]).stack().reset_index().rename(columns={\"level_2\": \"hour_block\", 0: \"energy_consumption\"})\n",
    "    #Creating a numerical hourblock column\n",
    "    block_df['offset'] = block_df['hour_block'].str.replace(\"hh_\", \"\").astype(int)\n",
    "    block_df_l.append(load_process_block_compact(block_df, freq=\"30min\", ts_identifier=\"LCLid\", value_name=\"energy_consumption\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2334663-867e-448e-82e3-8d796872b2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hhblock_df = pd.concat(block_df_l)\n",
    "del block_df_l\n",
    "display(hhblock_df.memory_usage(deep=True))\n",
    "print(f\"Total: {hhblock_df.memory_usage(deep=True).sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395b1b5-4f51-4ea4-979c-92720896e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhblock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329ca8-2d49-4ebd-a2dc-05399a4a616f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merging additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d04aba-d859-489e-81e5-dcef0caa0d96",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Household information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e77aa-99c0-407e-9ab6-9a59bc56bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "household_info = pd.read_csv(source_data/\"informations_households.csv\")\n",
    "household_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e047a646-7883-40d5-8423-bd469b0ab326",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhblock_df = hhblock_df.merge(household_info, on='LCLid', validate=\"one_to_one\")\n",
    "hhblock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa90fb-7fb6-4a08-a631-9a6d8ef86f40",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Weather and Bank Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73358282-4bf3-43ae-97f8-f96f4310ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_holidays = pd.read_csv(source_data/\"uk_bank_holidays.csv\", parse_dates=False)\n",
    "bank_holidays['Bank holidays'] = pd.to_datetime(bank_holidays['Bank holidays'], yearfirst=True)\n",
    "bank_holidays.set_index(\"Bank holidays\", inplace=True)\n",
    "bank_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d64c6-b29b-4f6e-8c89-793fe83ecc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reindex on standard date range\n",
    "bank_holidays = bank_holidays.resample(\"30min\").asfreq()\n",
    "bank_holidays = bank_holidays.groupby(bank_holidays.index.date).ffill().fillna(\"NO_HOLIDAY\")\n",
    "bank_holidays.index.name=\"datetime\"\n",
    "bank_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27a978-802a-4068-8c0c-eb3237886241",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_hourly = pd.read_csv(source_data/\"weather_hourly_darksky.csv\", parse_dates=False)\n",
    "weather_hourly['time'] = pd.to_datetime(weather_hourly['time'], yearfirst=True)\n",
    "weather_hourly.set_index(\"time\", inplace=True)\n",
    "weather_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10258a10-ca32-4e23-b1ec-eb430cee2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling at 30min and forward fill\n",
    "weather_hourly = weather_hourly.resample(\"30min\").ffill()\n",
    "weather_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034ec41-6e2b-42fd-ac0f-6532c93c436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_weather_holidays(row):\n",
    "    date_range = pd.date_range(row['start_timestamp'], periods=row['series_length'], freq=row['frequency'])\n",
    "    std_df = pd.DataFrame(index=date_range)\n",
    "    #Filling Na iwth NO_HOLIDAY cause rows before earliers holiday will be NaN\n",
    "    holidays = std_df.join(bank_holidays, how=\"left\").fillna(\"NO_HOLIDAY\")\n",
    "    weather = std_df.join(weather_hourly, how='left')\n",
    "    assert len(holidays)==row['series_length'], \"Length of holidays should be same as series length\"\n",
    "    assert len(weather)==row['series_length'], \"Length of weather should be same as series length\"\n",
    "    row['holidays'] = holidays['Type'].values\n",
    "    for col in weather:\n",
    "        row[col] = weather[col].values\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272753d-a258-4292-af01-691e538c8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhblock_df = hhblock_df.progress_apply(map_weather_holidays, axis=1)\n",
    "\n",
    "hhblock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d9e6f-00ce-4984-979d-1d0c44416b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del block_df, weather_hourly, bank_holidays, household_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a315b-3f17-4c82-801a-2c96da54ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hhblock_df.memory_usage(deep=True))\n",
    "print(f\"Total: {hhblock_df.memory_usage(deep=True).sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf0dba-a6a3-44df-bde0-44641466a322",
   "metadata": {},
   "source": [
    "### Saving the file on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080995b2-4895-4944-9f31-b5463d7935d0",
   "metadata": {},
   "source": [
    "Saving the entire dataset in a ts file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b67d9-3007-4eb0-9aeb-a21a3cc0161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/london_smart_meters/preprocessed\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627cbba-e116-4aa4-8be0-a72d1a5181c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a long time to finish. Comment out and execute only if needed\n",
    "# from src.utils.data_utils import write_compact_to_ts\n",
    "# write_compact_to_ts(hhblock_df,\n",
    "#        static_columns = ['LCLid', 'start_timestamp', 'frequency','series_length', 'stdorToU', 'Acorn', 'Acorn_grouped', 'file'], \n",
    "#        time_varying_columns = ['energy_consumption', 'holidays', 'visibility', 'windBearing', 'temperature', 'dewPoint',\n",
    "#                               'pressure', 'apparentTemperature', 'windSpeed', 'precipType', 'icon','humidity', 'summary'],\n",
    "#        filename=f\"data/london_smart_meters/preprocessed/london_smart_meters_merged.ts\",\n",
    "#        sep=\";\",\n",
    "#       chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616395a6-8d70-44ec-aa8f-36293cdebcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the LCLid - Acorn map as a pickle to be used later\n",
    "hhblock_df[['LCLid',\"file\", \"Acorn_grouped\"]].to_pickle(f\"data/london_smart_meters/preprocessed/london_smart_meters_lclid_acorn_map.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c637a5-99ac-421a-9562-a25ae0d71f13",
   "metadata": {},
   "source": [
    "Saving blocks in 8 chunks as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3f53c-11e4-4dcc-9449-a043419f7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the blocks into 8 chunks\n",
    "blocks = [f\"block_{i}\" for i in range(111)]\n",
    "\n",
    "n_chunks= 8\n",
    "split_blocks = [blocks[i:i + n_chunks] for i in range(0, len(blocks), n_chunks)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6235e-73cb-4c6e-a320-d5d77776f6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Writing each chunk to disk\n",
    "for blk in tqdm(split_blocks):\n",
    "    df = hhblock_df.loc[hhblock_df.file.isin(blk)]\n",
    "    blk = [int(b.replace(\"block_\",\"\")) for b in blk]\n",
    "    block_str = f\"block_{min(blk)}-{max(blk)}\"\n",
    "    df.to_parquet(f\"data/london_smart_meters/preprocessed/london_smart_meters_merged_{block_str}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "30fe1d27e07bcd39a201a380e0896b1a8ddc5b4c0f1bff527b499634c2e361cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
